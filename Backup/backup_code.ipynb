{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code that was previously used, but is no longer used.\n",
    "\n",
    "### Loop for training ensemble models\n",
    "```python\n",
    "\n",
    "plot_while_training = False\n",
    "\n",
    "def train_mlp(model, train_embeddings, train_labels, validation_embeddings, validation_labels, epochs=100, batch_size=2048, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    training_losses = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    # Create DataLoader for training data with shuffling\n",
    "    train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_embeddings, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_loss)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(validation_embeddings)\n",
    "            val_loss = criterion(val_outputs, validation_labels)\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            accuracy = (predicted == validation_labels).sum().item() / validation_labels.size(0)\n",
    "            validation_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Validation Loss: {val_loss.item():.4f}, Validation Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plotting training loss and validation accuracy\n",
    "    if plot_while_training:\n",
    "        epochs_range = range(1, epochs+1)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, training_losses, label='Training Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, [acc * 100 for acc in validation_accuracies], label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Validation Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return training_losses, validation_accuracies\n",
    "\n",
    "# Function to evaluate a single MLP\n",
    "def evaluate_mlp(model, embeddings, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(embeddings)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "    return accuracy, predicted.cpu().numpy()\n",
    "\n",
    "# Function to evaluate an ensemble of MLPs\n",
    "def evaluate_ensemble(ensemble, embeddings, labels, num_models=None):\n",
    "    ensemble_outputs = torch.zeros((embeddings.size(0), output_size)).to(device)\n",
    "    if num_models is None:\n",
    "        num_models = len(ensemble)\n",
    "    else:\n",
    "        num_models = min(num_models, len(ensemble))\n",
    "    \n",
    "    for model in ensemble[:num_models]:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(embeddings)\n",
    "            ensemble_outputs += outputs\n",
    "    \n",
    "    _, predicted = torch.max(ensemble_outputs, 1)\n",
    "    accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "    return accuracy, predicted.cpu().numpy()\n",
    "\n",
    "# Training and evaluating the ensemble\n",
    "validation_accuracies_per_model = []\n",
    "\n",
    "for idx, model in enumerate(ensemble, 1):\n",
    "    print(f\"\\nTraining model {idx}/{ensemble_size}\")\n",
    "    val_accuracy = train_mlp(model, train_embeddings_tensor, train_labels_tensor, validation_embeddings_tensor, validation_labels_tensor)\n",
    "    validation_accuracies_per_model.append(val_accuracy[1])\n",
    "\n",
    "# Plot validation accuracies for each ensemble member\n",
    "plt.figure()\n",
    "plt.plot(range(1, ensemble_size+1), [acc[-1] * 100 for acc in validation_accuracies_per_model], marker='o')\n",
    "plt.xlabel('Ensemble Member')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Validation Accuracy per Ensemble Member')\n",
    "plt.xticks(range(1, ensemble_size+1))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the ensemble on the validation set incrementally\n",
    "ensemble_sizes = range(1, ensemble_size + 1)\n",
    "ensemble_accuracies = []\n",
    "\n",
    "for n_models in ensemble_sizes:\n",
    "    accuracy, _ = evaluate_ensemble(ensemble, validation_embeddings_tensor, validation_labels_tensor, num_models=n_models)\n",
    "    ensemble_accuracies.append(accuracy)\n",
    "    print(f'Ensemble Size: {n_models}, Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot ensemble accuracy vs ensemble size\n",
    "plt.figure()\n",
    "plt.plot(ensemble_sizes, [acc * 100 for acc in ensemble_accuracies], marker='o')\n",
    "plt.xlabel('Number of Models in Ensemble')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Ensemble Validation Accuracy vs Ensemble Size')\n",
    "plt.xticks(ensemble_sizes)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics for the full ensemble\n",
    "ensemble_accuracy, ensemble_predicted = evaluate_ensemble(ensemble, validation_embeddings_tensor, validation_labels_tensor)\n",
    "print(f'\\nFull Ensemble Validation Accuracy: {ensemble_accuracy * 100:.2f}%')\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "validation_labels_np = validation_labels_tensor.cpu().numpy()\n",
    "\n",
    "print(\"\\nClassification Report for Full Ensemble:\")\n",
    "print(classification_report(validation_labels_np, ensemble_predicted, target_names=['Negative', 'Positive']))\n",
    "\n",
    "print(\"\\nConfusion Matrix for Full Ensemble:\")\n",
    "print(confusion_matrix(validation_labels_np, ensemble_predicted))\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
