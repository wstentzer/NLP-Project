{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B-Instruct\")\n",
    "\n",
    "# Move the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the SST-2 dataset from the GLUE benchmark\n",
    "dataset = load_dataset('glue', 'sst2', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\wsten\\miniconda3\\envs\\diffusion\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "67349it [24:19, 46.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (67349, 2048)\n",
      "Labels shape: (67349,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define tokenization function (works on individual examples)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Streaming dataset: iterate through the dataset manually\n",
    "streamed_dataset = dataset['train']\n",
    "\n",
    "# Initialize lists to store embeddings and labels\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Set a total number of examples to process if you want, or leave it for streaming\n",
    "total_examples = None  # Set a limit for demonstration purposes (or None for the whole dataset)\n",
    "\n",
    "# Iterate through the streamed dataset and tokenize + embed each example\n",
    "for i, example in enumerate(tqdm(streamed_dataset)):\n",
    "    \n",
    "    # Tokenize the example\n",
    "    tokenized_example = tokenize_function(example)\n",
    "    \n",
    "    # Convert tokenized inputs to PyTorch tensors\n",
    "    input_ids = torch.tensor(tokenized_example['input_ids']).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    attention_mask = torch.tensor(tokenized_example['attention_mask']).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embeddings using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Mean pooling with attention masking\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * expanded_attention_mask, dim=1)\n",
    "        sum_mask = torch.clamp(expanded_attention_mask.sum(dim=1), min=1e-9)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    # Store embeddings and labels\n",
    "    embeddings_list.append(embeddings.cpu().numpy())\n",
    "    labels_list.append(example['label'])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "labels = np.array(labels_list)\n",
    "\n",
    "# Print the shape of the embeddings to confirm successful processing\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP 1/10\n",
      "Epoch [1/10], Loss: 0.3690\n",
      "Epoch [2/10], Loss: 0.3778\n",
      "Epoch [3/10], Loss: 0.4081\n",
      "Epoch [4/10], Loss: 0.4425\n",
      "Epoch [5/10], Loss: 0.4217\n",
      "Epoch [6/10], Loss: 0.4552\n",
      "Epoch [7/10], Loss: 0.3827\n",
      "Epoch [8/10], Loss: 0.4605\n",
      "Epoch [9/10], Loss: 0.4865\n",
      "Epoch [10/10], Loss: 0.3618\n",
      "Training MLP 2/10\n",
      "Epoch [1/10], Loss: 0.4621\n",
      "Epoch [2/10], Loss: 0.4431\n",
      "Epoch [3/10], Loss: 0.4952\n",
      "Epoch [4/10], Loss: 0.4933\n",
      "Epoch [5/10], Loss: 0.3539\n",
      "Epoch [6/10], Loss: 0.3791\n",
      "Epoch [7/10], Loss: 0.3964\n",
      "Epoch [8/10], Loss: 0.5521\n",
      "Epoch [9/10], Loss: 0.4234\n",
      "Epoch [10/10], Loss: 0.3324\n",
      "Training MLP 3/10\n",
      "Epoch [1/10], Loss: 0.4155\n",
      "Epoch [2/10], Loss: 0.5022\n",
      "Epoch [3/10], Loss: 0.3344\n",
      "Epoch [4/10], Loss: 0.3875\n",
      "Epoch [5/10], Loss: 0.4528\n",
      "Epoch [6/10], Loss: 0.4954\n",
      "Epoch [7/10], Loss: 0.3693\n",
      "Epoch [8/10], Loss: 0.3672\n",
      "Epoch [9/10], Loss: 0.4187\n",
      "Epoch [10/10], Loss: 0.4182\n",
      "Training MLP 4/10\n",
      "Epoch [1/10], Loss: 0.3852\n",
      "Epoch [2/10], Loss: 0.4012\n",
      "Epoch [3/10], Loss: 0.5005\n",
      "Epoch [4/10], Loss: 0.4055\n",
      "Epoch [5/10], Loss: 0.3722\n",
      "Epoch [6/10], Loss: 0.4506\n",
      "Epoch [7/10], Loss: 0.3847\n",
      "Epoch [8/10], Loss: 0.3653\n",
      "Epoch [9/10], Loss: 0.4927\n",
      "Epoch [10/10], Loss: 0.3757\n",
      "Training MLP 5/10\n",
      "Epoch [1/10], Loss: 0.3942\n",
      "Epoch [2/10], Loss: 0.4380\n",
      "Epoch [3/10], Loss: 0.4718\n",
      "Epoch [4/10], Loss: 0.3935\n",
      "Epoch [5/10], Loss: 0.4264\n",
      "Epoch [6/10], Loss: 0.3592\n",
      "Epoch [7/10], Loss: 0.4138\n",
      "Epoch [8/10], Loss: 0.4834\n",
      "Epoch [9/10], Loss: 0.3297\n",
      "Epoch [10/10], Loss: 0.4080\n",
      "Training MLP 6/10\n",
      "Epoch [1/10], Loss: 0.3631\n",
      "Epoch [2/10], Loss: 0.3693\n",
      "Epoch [3/10], Loss: 0.4838\n",
      "Epoch [4/10], Loss: 0.4601\n",
      "Epoch [5/10], Loss: 0.5043\n",
      "Epoch [6/10], Loss: 0.3245\n",
      "Epoch [7/10], Loss: 0.4070\n",
      "Epoch [8/10], Loss: 0.3416\n",
      "Epoch [9/10], Loss: 0.3617\n",
      "Epoch [10/10], Loss: 0.4279\n",
      "Training MLP 7/10\n",
      "Epoch [1/10], Loss: 0.4731\n",
      "Epoch [2/10], Loss: 0.4445\n",
      "Epoch [3/10], Loss: 0.4125\n",
      "Epoch [4/10], Loss: 0.4105\n",
      "Epoch [5/10], Loss: 0.4917\n",
      "Epoch [6/10], Loss: 0.4085\n",
      "Epoch [7/10], Loss: 0.4456\n",
      "Epoch [8/10], Loss: 0.3689\n",
      "Epoch [9/10], Loss: 0.4547\n",
      "Epoch [10/10], Loss: 0.4635\n",
      "Training MLP 8/10\n",
      "Epoch [1/10], Loss: 0.4425\n",
      "Epoch [2/10], Loss: 0.5364\n",
      "Epoch [3/10], Loss: 0.5257\n",
      "Epoch [4/10], Loss: 0.4484\n",
      "Epoch [5/10], Loss: 0.6018\n",
      "Epoch [6/10], Loss: 0.3856\n",
      "Epoch [7/10], Loss: 0.4515\n",
      "Epoch [8/10], Loss: 0.3792\n",
      "Epoch [9/10], Loss: 0.5058\n",
      "Epoch [10/10], Loss: 0.4120\n",
      "Training MLP 9/10\n",
      "Epoch [1/10], Loss: 0.3641\n",
      "Epoch [2/10], Loss: 0.4568\n",
      "Epoch [3/10], Loss: 0.4093\n",
      "Epoch [4/10], Loss: 0.3614\n",
      "Epoch [5/10], Loss: 0.3764\n",
      "Epoch [6/10], Loss: 0.3695\n",
      "Epoch [7/10], Loss: 0.4397\n",
      "Epoch [8/10], Loss: 0.3818\n",
      "Epoch [9/10], Loss: 0.3672\n",
      "Epoch [10/10], Loss: 0.4707\n",
      "Training MLP 10/10\n",
      "Epoch [1/10], Loss: 0.4040\n",
      "Epoch [2/10], Loss: 0.5380\n",
      "Epoch [3/10], Loss: 0.4083\n",
      "Epoch [4/10], Loss: 0.3135\n",
      "Epoch [5/10], Loss: 0.3907\n",
      "Epoch [6/10], Loss: 0.3320\n",
      "Epoch [7/10], Loss: 0.4445\n",
      "Epoch [8/10], Loss: 0.3746\n",
      "Epoch [9/10], Loss: 0.3816\n",
      "Epoch [10/10], Loss: 0.3926\n",
      "Predictive variance for first few samples: [[6.2016567e-04 6.2016555e-04]\n",
      " [2.1361142e-05 2.1361266e-05]\n",
      " [2.8604026e-30 0.0000000e+00]\n",
      " [4.9819651e-16 1.4210855e-15]\n",
      " [7.9947205e-07 7.9947131e-07]]\n",
      "Predictive entropy for first few samples: [7.4693501e-02 1.2246697e-02 1.1683351e-14 2.3039902e-07 4.8157596e-03]\n"
     ]
    }
   ],
   "source": [
    "# Define the MLP architecture\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Create an ensemble of MLPs\n",
    "ensemble_size = 10  # Set ensemble size\n",
    "input_size = embeddings.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 2  # For binary classification\n",
    "\n",
    "ensemble = [MLPClassifier(input_size, hidden_size, output_size).to(device) for _ in range(ensemble_size)]\n",
    "\n",
    "# Convert embeddings and labels to tensors\n",
    "train_embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "train_labels_tensor = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "# Training loop for MLP classifiers\n",
    "def train_mlp(model, train_embeddings, train_labels, epochs=10, learning_rate=0.001, batch_size=32):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    dataset_size = train_embeddings.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Shuffle the data\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        train_embeddings = train_embeddings[indices]\n",
    "        train_labels = train_labels[indices]\n",
    "        \n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_embeddings = train_embeddings[i:i+batch_size]\n",
    "            batch_labels = train_labels[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Train each MLP in the ensemble\n",
    "for i, model in enumerate(ensemble):\n",
    "    print(f'Training MLP {i+1}/{ensemble_size}')\n",
    "    train_mlp(model, train_embeddings_tensor, train_labels_tensor)\n",
    "\n",
    "# Function for ensemble prediction with uncertainty quantification\n",
    "def ensemble_predict_with_uncertainty(ensemble, embeddings, batch_size=32):\n",
    "    ensemble_predictions = []\n",
    "    dataset_size = embeddings.shape[0]\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    for i in range(0, dataset_size, batch_size):\n",
    "        batch_embeddings = embeddings_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Collect predictions from each model in the ensemble\n",
    "        batch_predictions = []\n",
    "        for model in ensemble:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_embeddings)\n",
    "                batch_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        # Stack all predictions for the current batch and append to the ensemble predictions\n",
    "        batch_predictions = np.stack(batch_predictions)\n",
    "        ensemble_predictions.append(batch_predictions)\n",
    "\n",
    "    # Stack all batch-level predictions across all samples\n",
    "    ensemble_predictions = np.concatenate(ensemble_predictions, axis=1)\n",
    "\n",
    "    # Average predictions across ensemble members\n",
    "    averaged_predictions = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive uncertainty (variance across ensemble predictions)\n",
    "    predictive_variance = np.var(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive entropy (measures uncertainty)\n",
    "    predictive_entropy = -np.sum(averaged_predictions * np.log(averaged_predictions + 1e-9), axis=1)\n",
    "\n",
    "    # Final class predictions (based on averaged predictions)\n",
    "    final_predictions = np.argmax(averaged_predictions, axis=1)\n",
    "\n",
    "    return final_predictions, predictive_variance, predictive_entropy\n",
    "\n",
    "# Example: Get ensemble predictions on the training set\n",
    "ensemble_predictions, predictive_variance, predictive_entropy = ensemble_predict_with_uncertainty(ensemble, embeddings)\n",
    "\n",
    "# Print predictive uncertainty for the first few samples\n",
    "print(f'Predictive variance for first few samples: {predictive_variance[:5]}')\n",
    "print(f'Predictive entropy for first few samples: {predictive_entropy[:5]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive variance for first few samples: [[6.2016567e-04 6.2016555e-04]\n",
      " [2.1361142e-05 2.1361266e-05]\n",
      " [2.8604026e-30 0.0000000e+00]\n",
      " [4.9819651e-16 1.4210855e-15]\n",
      " [7.9947205e-07 7.9947131e-07]]\n",
      "Predictive entropy for first few samples: [7.4693501e-02 1.2246697e-02 1.1683351e-14 2.3039902e-07 4.8157596e-03]\n",
      "Ensemble Accuracy: 93.62%\n"
     ]
    }
   ],
   "source": [
    "# Function for ensemble prediction with uncertainty quantification and accuracy calculation\n",
    "def ensemble_predict_with_uncertainty(ensemble, embeddings, true_labels, batch_size=32):\n",
    "    ensemble_predictions = []\n",
    "    dataset_size = embeddings.shape[0]\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    for i in range(0, dataset_size, batch_size):\n",
    "        batch_embeddings = embeddings_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Collect predictions from each model in the ensemble\n",
    "        batch_predictions = []\n",
    "        for model in ensemble:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_embeddings)\n",
    "                batch_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        # Stack all predictions for the current batch and append to the ensemble predictions\n",
    "        batch_predictions = np.stack(batch_predictions)\n",
    "        ensemble_predictions.append(batch_predictions)\n",
    "\n",
    "    # Stack all batch-level predictions across all samples\n",
    "    ensemble_predictions = np.concatenate(ensemble_predictions, axis=1)\n",
    "\n",
    "    # Average predictions across ensemble members\n",
    "    averaged_predictions = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive uncertainty (variance across ensemble predictions)\n",
    "    predictive_variance = np.var(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive entropy (measures uncertainty)\n",
    "    predictive_entropy = -np.sum(averaged_predictions * np.log(averaged_predictions + 1e-9), axis=1)\n",
    "\n",
    "    # Final class predictions (based on averaged predictions)\n",
    "    final_predictions = np.argmax(averaged_predictions, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(final_predictions == true_labels)\n",
    "    \n",
    "    return final_predictions, predictive_variance, predictive_entropy, accuracy\n",
    "\n",
    "# Example: Get ensemble predictions on the training set and calculate accuracy\n",
    "ensemble_predictions, predictive_variance, predictive_entropy, accuracy = ensemble_predict_with_uncertainty(ensemble, embeddings, labels)\n",
    "\n",
    "# Print predictive uncertainty for the first few samples\n",
    "print(f'Predictive variance for first few samples: {predictive_variance[:5]}')\n",
    "print(f'Predictive entropy for first few samples: {predictive_entropy[:5]}')\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Ensemble Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
