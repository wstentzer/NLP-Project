{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d086402108a43bb8f66cf8d256a7a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wsten\\miniconda3\\envs\\diffusion\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wsten\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68acfe82333c4d31bf803467b6c239f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a629f98cb474a55b3c17db564e346a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fe80ebf9654c128a10dd1d3bc8e853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48876bb27ef646798df7ca762b0f1e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f01f04c96fb446183cfc0f3df9523ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3690c59e204f1c8b6efa9db9c222c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c3b5d71ad84b16b84c3a002b6e6e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6b20f855fd4c7eaaf82b6a4309b495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49b459d6f60488d850be4fd8ec47453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cd08bb812d4b7b88f58eda7347afe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99845e1cf924a7b94a439fdc455bdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd48db22eef641f5a72d518790294df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5756c49f7934e5c97452dac8dec8cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True)\n",
    "\n",
    "# Move the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the SST-2 dataset from the GLUE benchmark\n",
    "dataset = load_dataset('glue', 'sst2', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
      "67349it [56:12, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (67349, 3072)\n",
      "Labels shape: (67349,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define tokenization function (works on individual examples)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Streaming dataset: iterate through the dataset manually\n",
    "streamed_dataset = dataset['train']\n",
    "\n",
    "# Initialize lists to store embeddings and labels\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Iterate through the streamed dataset and tokenize + embed each example\n",
    "for i, example in enumerate(tqdm(streamed_dataset)):\n",
    "    \n",
    "    # Tokenize the example\n",
    "    tokenized_example = tokenize_function(example)\n",
    "    \n",
    "    # Convert tokenized inputs to PyTorch tensors\n",
    "    input_ids = torch.tensor(tokenized_example['input_ids']).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    attention_mask = torch.tensor(tokenized_example['attention_mask']).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embeddings using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Mean pooling with attention masking\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * expanded_attention_mask, dim=1)\n",
    "        sum_mask = torch.clamp(expanded_attention_mask.sum(dim=1), min=1e-9)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    # Store embeddings and labels\n",
    "    embeddings_list.append(embeddings.cpu().numpy())\n",
    "    labels_list.append(example['label'])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "labels = np.array(labels_list)\n",
    "\n",
    "# Print the shape of the embeddings to confirm successful processing\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP 1/10\n",
      "Epoch [1/10], Loss: 0.5990\n",
      "Epoch [2/10], Loss: 0.7895\n",
      "Epoch [3/10], Loss: 0.7895\n",
      "Epoch [4/10], Loss: 0.7418\n",
      "Epoch [5/10], Loss: 0.7895\n",
      "Epoch [6/10], Loss: 0.8371\n",
      "Epoch [7/10], Loss: 0.6466\n",
      "Epoch [8/10], Loss: 0.7418\n",
      "Epoch [9/10], Loss: 0.7418\n",
      "Epoch [10/10], Loss: 0.6942\n",
      "Training MLP 2/10\n",
      "Epoch [1/10], Loss: 0.6466\n",
      "Epoch [2/10], Loss: 0.6942\n",
      "Epoch [3/10], Loss: 0.6466\n",
      "Epoch [4/10], Loss: 0.7895\n",
      "Epoch [5/10], Loss: 0.5990\n",
      "Epoch [6/10], Loss: 0.6466\n",
      "Epoch [7/10], Loss: 0.7418\n",
      "Epoch [8/10], Loss: 0.6942\n",
      "Epoch [9/10], Loss: 0.5990\n",
      "Epoch [10/10], Loss: 0.7895\n",
      "Training MLP 3/10\n",
      "Epoch [1/10], Loss: 0.6942\n",
      "Epoch [2/10], Loss: 0.8371\n",
      "Epoch [3/10], Loss: 0.7418\n",
      "Epoch [4/10], Loss: 0.9799\n",
      "Epoch [5/10], Loss: 0.7418\n",
      "Epoch [6/10], Loss: 0.7895\n",
      "Epoch [7/10], Loss: 0.8847\n",
      "Epoch [8/10], Loss: 0.7418\n",
      "Epoch [9/10], Loss: 0.9323\n",
      "Epoch [10/10], Loss: 0.7418\n",
      "Training MLP 4/10\n",
      "Epoch [1/10], Loss: 0.8847\n",
      "Epoch [2/10], Loss: 0.8371\n",
      "Epoch [3/10], Loss: 0.9323\n",
      "Epoch [4/10], Loss: 0.7418\n",
      "Epoch [5/10], Loss: 1.0275\n",
      "Epoch [6/10], Loss: 0.9799\n",
      "Epoch [7/10], Loss: 0.8371\n",
      "Epoch [8/10], Loss: 1.0275\n",
      "Epoch [9/10], Loss: 0.9799\n",
      "Epoch [10/10], Loss: 1.0275\n",
      "Training MLP 5/10\n",
      "Epoch [1/10], Loss: 0.4517\n",
      "Epoch [2/10], Loss: 0.3530\n",
      "Epoch [3/10], Loss: 0.4446\n",
      "Epoch [4/10], Loss: 0.3150\n",
      "Epoch [5/10], Loss: 0.4311\n",
      "Epoch [6/10], Loss: 0.4196\n",
      "Epoch [7/10], Loss: 0.3627\n",
      "Epoch [8/10], Loss: 0.3314\n",
      "Epoch [9/10], Loss: 0.4414\n",
      "Epoch [10/10], Loss: 0.4490\n",
      "Training MLP 6/10\n",
      "Epoch [1/10], Loss: 0.8371\n",
      "Epoch [2/10], Loss: 0.5514\n",
      "Epoch [3/10], Loss: 0.6466\n",
      "Epoch [4/10], Loss: 0.7418\n",
      "Epoch [5/10], Loss: 0.7418\n",
      "Epoch [6/10], Loss: 0.8371\n",
      "Epoch [7/10], Loss: 0.9799\n",
      "Epoch [8/10], Loss: 0.8847\n",
      "Epoch [9/10], Loss: 0.5037\n",
      "Epoch [10/10], Loss: 0.6466\n",
      "Training MLP 7/10\n",
      "Epoch [1/10], Loss: 0.9323\n",
      "Epoch [2/10], Loss: 0.5990\n",
      "Epoch [3/10], Loss: 0.8371\n",
      "Epoch [4/10], Loss: 0.7895\n",
      "Epoch [5/10], Loss: 0.5037\n",
      "Epoch [6/10], Loss: 0.9323\n",
      "Epoch [7/10], Loss: 0.6466\n",
      "Epoch [8/10], Loss: 0.8847\n",
      "Epoch [9/10], Loss: 0.5990\n",
      "Epoch [10/10], Loss: 0.7895\n",
      "Training MLP 8/10\n",
      "Epoch [1/10], Loss: 0.4107\n",
      "Epoch [2/10], Loss: 0.4745\n",
      "Epoch [3/10], Loss: 0.4034\n",
      "Epoch [4/10], Loss: 0.4234\n",
      "Epoch [5/10], Loss: 0.3927\n",
      "Epoch [6/10], Loss: 0.3637\n",
      "Epoch [7/10], Loss: 0.3442\n",
      "Epoch [8/10], Loss: 0.4583\n",
      "Epoch [9/10], Loss: 0.3151\n",
      "Epoch [10/10], Loss: 0.3644\n",
      "Training MLP 9/10\n",
      "Epoch [1/10], Loss: 0.5990\n",
      "Epoch [2/10], Loss: 0.8371\n",
      "Epoch [3/10], Loss: 1.0275\n",
      "Epoch [4/10], Loss: 0.5514\n",
      "Epoch [5/10], Loss: 0.6466\n",
      "Epoch [6/10], Loss: 0.7418\n",
      "Epoch [7/10], Loss: 0.7418\n",
      "Epoch [8/10], Loss: 0.6466\n",
      "Epoch [9/10], Loss: 0.6942\n",
      "Epoch [10/10], Loss: 0.8371\n",
      "Training MLP 10/10\n",
      "Epoch [1/10], Loss: 0.6942\n",
      "Epoch [2/10], Loss: 0.8371\n",
      "Epoch [3/10], Loss: 0.8371\n",
      "Epoch [4/10], Loss: 0.5990\n",
      "Epoch [5/10], Loss: 0.7418\n",
      "Epoch [6/10], Loss: 0.6942\n",
      "Epoch [7/10], Loss: 0.7418\n",
      "Epoch [8/10], Loss: 0.8371\n",
      "Epoch [9/10], Loss: 0.6942\n",
      "Epoch [10/10], Loss: 0.8371\n",
      "Predictive variance for first few samples: [[0.18087845 0.18087845]\n",
      " [0.20999369 0.20999369]\n",
      " [0.08999999 0.08999999]\n",
      " [0.08998967 0.08998967]\n",
      " [0.20995176 0.20995176]]\n",
      "Predictive entropy for first few samples: [0.5895269  0.61086047 0.325083   0.32519665 0.6108351 ]\n"
     ]
    }
   ],
   "source": [
    "# Define the MLP architecture\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Create an ensemble of MLPs\n",
    "ensemble_size = 10  # Set ensemble size\n",
    "input_size = embeddings.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 2  # For binary classification\n",
    "\n",
    "ensemble = [MLPClassifier(input_size, hidden_size, output_size).to(device) for _ in range(ensemble_size)]\n",
    "\n",
    "# Convert embeddings and labels to tensors\n",
    "train_embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "train_labels_tensor = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "# Training loop for MLP classifiers\n",
    "def train_mlp(model, train_embeddings, train_labels, epochs=10, learning_rate=0.001, batch_size=32):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    dataset_size = train_embeddings.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Shuffle the data\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        train_embeddings = train_embeddings[indices]\n",
    "        train_labels = train_labels[indices]\n",
    "        \n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_embeddings = train_embeddings[i:i+batch_size]\n",
    "            batch_labels = train_labels[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Train each MLP in the ensemble\n",
    "for i, model in enumerate(ensemble):\n",
    "    print(f'Training MLP {i+1}/{ensemble_size}')\n",
    "    train_mlp(model, train_embeddings_tensor, train_labels_tensor)\n",
    "\n",
    "# Function for ensemble prediction with uncertainty quantification\n",
    "def ensemble_predict_with_uncertainty(ensemble, embeddings, batch_size=32):\n",
    "    ensemble_predictions = []\n",
    "    dataset_size = embeddings.shape[0]\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    for i in range(0, dataset_size, batch_size):\n",
    "        batch_embeddings = embeddings_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Collect predictions from each model in the ensemble\n",
    "        batch_predictions = []\n",
    "        for model in ensemble:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_embeddings)\n",
    "                batch_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        # Stack all predictions for the current batch and append to the ensemble predictions\n",
    "        batch_predictions = np.stack(batch_predictions)\n",
    "        ensemble_predictions.append(batch_predictions)\n",
    "\n",
    "    # Stack all batch-level predictions across all samples\n",
    "    ensemble_predictions = np.concatenate(ensemble_predictions, axis=1)\n",
    "\n",
    "    # Average predictions across ensemble members\n",
    "    averaged_predictions = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive uncertainty (variance across ensemble predictions)\n",
    "    predictive_variance = np.var(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive entropy (measures uncertainty)\n",
    "    predictive_entropy = -np.sum(averaged_predictions * np.log(averaged_predictions + 1e-9), axis=1)\n",
    "\n",
    "    # Final class predictions (based on averaged predictions)\n",
    "    final_predictions = np.argmax(averaged_predictions, axis=1)\n",
    "\n",
    "    return final_predictions, predictive_variance, predictive_entropy\n",
    "\n",
    "# Example: Get ensemble predictions on the training set\n",
    "ensemble_predictions, predictive_variance, predictive_entropy = ensemble_predict_with_uncertainty(ensemble, embeddings)\n",
    "\n",
    "# Print predictive uncertainty for the first few samples\n",
    "print(f'Predictive variance for first few samples: {predictive_variance[:5]}')\n",
    "print(f'Predictive entropy for first few samples: {predictive_entropy[:5]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive variance for first few samples: [[0.18087845 0.18087845]\n",
      " [0.20999369 0.20999369]\n",
      " [0.08999999 0.08999999]\n",
      " [0.08998967 0.08998967]\n",
      " [0.20995176 0.20995176]]\n",
      "Predictive entropy for first few samples: [0.5895269  0.61086047 0.325083   0.32519665 0.6108351 ]\n",
      "Ensemble Accuracy: 55.78%\n"
     ]
    }
   ],
   "source": [
    "# Function for ensemble prediction with uncertainty quantification and accuracy calculation\n",
    "def ensemble_predict_with_uncertainty(ensemble, embeddings, true_labels, batch_size=32):\n",
    "    ensemble_predictions = []\n",
    "    dataset_size = embeddings.shape[0]\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    for i in range(0, dataset_size, batch_size):\n",
    "        batch_embeddings = embeddings_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Collect predictions from each model in the ensemble\n",
    "        batch_predictions = []\n",
    "        for model in ensemble:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_embeddings)\n",
    "                batch_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        # Stack all predictions for the current batch and append to the ensemble predictions\n",
    "        batch_predictions = np.stack(batch_predictions)\n",
    "        ensemble_predictions.append(batch_predictions)\n",
    "\n",
    "    # Stack all batch-level predictions across all samples\n",
    "    ensemble_predictions = np.concatenate(ensemble_predictions, axis=1)\n",
    "\n",
    "    # Average predictions across ensemble members\n",
    "    averaged_predictions = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive uncertainty (variance across ensemble predictions)\n",
    "    predictive_variance = np.var(ensemble_predictions, axis=0)\n",
    "\n",
    "    # Calculate predictive entropy (measures uncertainty)\n",
    "    predictive_entropy = -np.sum(averaged_predictions * np.log(averaged_predictions + 1e-9), axis=1)\n",
    "\n",
    "    # Final class predictions (based on averaged predictions)\n",
    "    final_predictions = np.argmax(averaged_predictions, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(final_predictions == true_labels)\n",
    "    \n",
    "    return final_predictions, predictive_variance, predictive_entropy, accuracy\n",
    "\n",
    "# Example: Get ensemble predictions on the training set and calculate accuracy\n",
    "ensemble_predictions, predictive_variance, predictive_entropy, accuracy = ensemble_predict_with_uncertainty(ensemble, embeddings, labels)\n",
    "\n",
    "# Print predictive uncertainty for the first few samples\n",
    "print(f'Predictive variance for first few samples: {predictive_variance[:5]}')\n",
    "print(f'Predictive entropy for first few samples: {predictive_entropy[:5]}')\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Ensemble Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
